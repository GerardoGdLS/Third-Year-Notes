{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cfb43af",
   "metadata": {},
   "source": [
    "# Assignment 4: Pipelines and Text Data (60 total marks)\n",
    "### Due: March 21 at 11:59pm\n",
    "\n",
    "### Name: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "165f8ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "5ff9ad29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore') #ignoring some deprication warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf05c7e",
   "metadata": {},
   "source": [
    "## Part 1: Pipelines (26 marks)\n",
    "\n",
    "The purpose of this part of the assignment is to practice following the grid-search workflow: \n",
    "- Split data into training and test set\n",
    "- Use the training portion to find the best model using grid search and cross-validation\n",
    "- Retrain the best model\n",
    "- Evaluate the retrained model on the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31108625",
   "metadata": {},
   "source": [
    "### 1.1: Load data (4 marks)\n",
    "For this task, we will be using stock data from the Dow Jones Index. This dataset uses information about different stocks to try to predict what the percent change in price will be from week to week.\n",
    "\n",
    "More information on the dataset can be found here: https://archive.ics.uci.edu/dataset/312/dow+jones+index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "5b1e2cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   quarter stock       date    open    high     low   close     volume  \\\n",
      "0        1    AA   1/7/2011  $15.82  $16.72  $15.78  $16.42  239655616   \n",
      "1        1    AA  1/14/2011  $16.71  $16.71  $15.64  $15.97  242963398   \n",
      "2        1    AA  1/21/2011  $16.19  $16.38  $15.60  $15.79  138428495   \n",
      "3        1    AA  1/28/2011  $15.87  $16.63  $15.82  $16.13  151379173   \n",
      "4        1    AA   2/4/2011  $16.18  $17.39  $16.18  $17.14  154387761   \n",
      "\n",
      "   percent_change_price  percent_change_volume_over_last_wk  \\\n",
      "0               3.79267                                 NaN   \n",
      "1              -4.42849                            1.380223   \n",
      "2              -2.47066                          -43.024959   \n",
      "3               1.63831                            9.355500   \n",
      "4               5.93325                            1.987452   \n",
      "\n",
      "   previous_weeks_volume next_weeks_open next_weeks_close  \\\n",
      "0                    NaN          $16.71           $15.97   \n",
      "1            239655616.0          $16.19           $15.79   \n",
      "2            242963398.0          $15.87           $16.13   \n",
      "3            138428495.0          $16.18           $17.14   \n",
      "4            151379173.0          $17.33           $17.37   \n",
      "\n",
      "   percent_change_next_weeks_price  days_to_next_dividend  \\\n",
      "0                        -4.428490                     26   \n",
      "1                        -2.470660                     19   \n",
      "2                         1.638310                     12   \n",
      "3                         5.933250                      5   \n",
      "4                         0.230814                     97   \n",
      "\n",
      "   percent_return_next_dividend  \n",
      "0                      0.182704  \n",
      "1                      0.187852  \n",
      "2                      0.189994  \n",
      "3                      0.185989  \n",
      "4                      0.175029  \n"
     ]
    }
   ],
   "source": [
    "# TO DO: Load the dataset into a dataframe called stock_data (0.5 marks)\n",
    "stock_data = pd.read_csv('dow_jones_index.data')\n",
    "\n",
    "# TO DO: Inspect the first few columns (0.5 marks)\n",
    "\n",
    "print(stock_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "8969a634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quarter                                 int64\n",
      "stock                                  object\n",
      "date                                   object\n",
      "open                                   object\n",
      "high                                   object\n",
      "low                                    object\n",
      "close                                  object\n",
      "volume                                  int64\n",
      "percent_change_price                  float64\n",
      "percent_change_volume_over_last_wk    float64\n",
      "previous_weeks_volume                 float64\n",
      "next_weeks_open                        object\n",
      "next_weeks_close                       object\n",
      "percent_change_next_weeks_price       float64\n",
      "days_to_next_dividend                   int64\n",
      "percent_return_next_dividend          float64\n",
      "dtype: object\n",
      "quarter                                0\n",
      "stock                                  0\n",
      "date                                   0\n",
      "open                                   0\n",
      "high                                   0\n",
      "low                                    0\n",
      "close                                  0\n",
      "volume                                 0\n",
      "percent_change_price                   0\n",
      "percent_change_volume_over_last_wk    30\n",
      "previous_weeks_volume                 30\n",
      "next_weeks_open                        0\n",
      "next_weeks_close                       0\n",
      "percent_change_next_weeks_price        0\n",
      "days_to_next_dividend                  0\n",
      "percent_return_next_dividend           0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# TO DO: Check the data types of each column and if there are missing values (0.5 marks)\n",
    "\n",
    "print(stock_data.dtypes)\n",
    "print(stock_data.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17914e32",
   "metadata": {},
   "source": [
    "You should notice in this dataset that there are multiple columns that look numerical, but include a `$` that turns the value into a string (type object). You can use the code below to convert these columns into numerical ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "d222ca1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   quarter stock       date   open   high    low  close     volume  \\\n",
      "0        1    AA   1/7/2011  15.82  16.72  15.78  16.42  239655616   \n",
      "1        1    AA  1/14/2011  16.71  16.71  15.64  15.97  242963398   \n",
      "2        1    AA  1/21/2011  16.19  16.38  15.60  15.79  138428495   \n",
      "3        1    AA  1/28/2011  15.87  16.63  15.82  16.13  151379173   \n",
      "4        1    AA   2/4/2011  16.18  17.39  16.18  17.14  154387761   \n",
      "\n",
      "   percent_change_price  percent_change_volume_over_last_wk  \\\n",
      "0               3.79267                                 NaN   \n",
      "1              -4.42849                            1.380223   \n",
      "2              -2.47066                          -43.024959   \n",
      "3               1.63831                            9.355500   \n",
      "4               5.93325                            1.987452   \n",
      "\n",
      "   previous_weeks_volume  next_weeks_open  next_weeks_close  \\\n",
      "0                    NaN            16.71             15.97   \n",
      "1            239655616.0            16.19             15.79   \n",
      "2            242963398.0            15.87             16.13   \n",
      "3            138428495.0            16.18             17.14   \n",
      "4            151379173.0            17.33             17.37   \n",
      "\n",
      "   percent_change_next_weeks_price  days_to_next_dividend  \\\n",
      "0                        -4.428490                     26   \n",
      "1                        -2.470660                     19   \n",
      "2                         1.638310                     12   \n",
      "3                         5.933250                      5   \n",
      "4                         0.230814                     97   \n",
      "\n",
      "   percent_return_next_dividend  \n",
      "0                      0.182704  \n",
      "1                      0.187852  \n",
      "2                      0.189994  \n",
      "3                      0.185989  \n",
      "4                      0.175029  \n"
     ]
    }
   ],
   "source": [
    "# TO DO: Fill-in which columns need the $ to be removed (1 mark)\n",
    "columns = ['open', 'high', 'low', 'close', 'next_weeks_open', 'next_weeks_close']   \n",
    "\n",
    "# Code to remove $ - DO NOT CHANGE\n",
    "stock_data[columns] = stock_data[columns].replace('[\\$]', '', regex=True).astype(float)\n",
    "\n",
    "# TO DO: Inspect first few rows to make sure it worked (0.5 marks)\n",
    "print(stock_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "32ad28a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quarter                                 int64\n",
      "stock                                  object\n",
      "date                                   object\n",
      "open                                  float64\n",
      "high                                  float64\n",
      "low                                   float64\n",
      "close                                 float64\n",
      "volume                                  int64\n",
      "percent_change_price                  float64\n",
      "percent_change_volume_over_last_wk    float64\n",
      "previous_weeks_volume                 float64\n",
      "next_weeks_open                       float64\n",
      "next_weeks_close                      float64\n",
      "percent_change_next_weeks_price       float64\n",
      "days_to_next_dividend                   int64\n",
      "percent_return_next_dividend          float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# TO DO: Check data type of each column to make sure that the type of the columns selected has changed (0.5 marks)\n",
    "print(stock_data.dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dac4013",
   "metadata": {},
   "source": [
    "The first thing we need to do is deal with missing values. Looking at the dataset, there are two columns with 30 missing values. For this case, we will drop these rows instead of filling them in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "9fcd2350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   quarter stock       date   open   high    low  close     volume  \\\n",
      "1        1    AA  1/14/2011  16.71  16.71  15.64  15.97  242963398   \n",
      "2        1    AA  1/21/2011  16.19  16.38  15.60  15.79  138428495   \n",
      "3        1    AA  1/28/2011  15.87  16.63  15.82  16.13  151379173   \n",
      "4        1    AA   2/4/2011  16.18  17.39  16.18  17.14  154387761   \n",
      "5        1    AA  2/11/2011  17.33  17.48  16.97  17.37  114691279   \n",
      "\n",
      "   percent_change_price  percent_change_volume_over_last_wk  \\\n",
      "1             -4.428490                            1.380223   \n",
      "2             -2.470660                          -43.024959   \n",
      "3              1.638310                            9.355500   \n",
      "4              5.933250                            1.987452   \n",
      "5              0.230814                          -25.712195   \n",
      "\n",
      "   previous_weeks_volume  next_weeks_open  next_weeks_close  \\\n",
      "1            239655616.0            16.19             15.79   \n",
      "2            242963398.0            15.87             16.13   \n",
      "3            138428495.0            16.18             17.14   \n",
      "4            151379173.0            17.33             17.37   \n",
      "5            154387761.0            17.39             17.28   \n",
      "\n",
      "   percent_change_next_weeks_price  days_to_next_dividend  \\\n",
      "1                        -2.470660                     19   \n",
      "2                         1.638310                     12   \n",
      "3                         5.933250                      5   \n",
      "4                         0.230814                     97   \n",
      "5                        -0.632547                     90   \n",
      "\n",
      "   percent_return_next_dividend  \n",
      "1                      0.187852  \n",
      "2                      0.189994  \n",
      "3                      0.185989  \n",
      "4                      0.175029  \n",
      "5                      0.172712  \n"
     ]
    }
   ],
   "source": [
    "# TO DO: Drop rows with missing data (0.5 marks)\n",
    "\n",
    "stock_data = stock_data.dropna()\n",
    "\n",
    "print(stock_data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b10d14a",
   "metadata": {},
   "source": [
    "### 1.2: Pre-processing (4 marks)\n",
    "\n",
    "In this dataset, we have columns with:\n",
    "- Categorical values\n",
    "- Numerical values\n",
    "\n",
    "We need to create a column transformer that will use the proper preprocessing methods on each type of column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "89e30a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Create Column Transformer using an encoder and StandardScaler (1 mark)\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "ct = ColumnTransformer(\n",
    "    [(\"scaling\", StandardScaler(), ['quarter', 'open', 'high', 'low', 'close', 'volume', 'percent_change_price', 'next_weeks_open', 'next_weeks_close', 'percent_change_next_weeks_price', 'days_to_next_dividend', 'percent_return_next_dividend']),\n",
    "     (\"onehot\", OneHotEncoder(sparse_output=False), ['stock', 'date'])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "38f08685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Initialize your pipeline with your column transformer and the Ridge Regression model (1 mark)\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "pipe = Pipeline(steps=[('preprocessor', ct), ('classifier', Ridge())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "c154f735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Separate data into feature matrix and target vector (1 mark)\n",
    "\n",
    "X = stock_data.drop('percent_change_next_weeks_price', axis=1)  \n",
    "y = stock_data['percent_change_next_weeks_price']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "83d815ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Split data into training and testing sets (use random_state=0 and 10% of the data for testing) (0.5 marks)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e626fe",
   "metadata": {},
   "source": [
    "Create another column transformer that does not implement scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "39312544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Create a new column transformer that only performs encoding (0.5 marks)\n",
    "\n",
    "ct_encoding = ColumnTransformer(\n",
    "    [(\"onehot\", OneHotEncoder(sparse_output=False), ['stock', 'date'])])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd751b7a",
   "metadata": {},
   "source": [
    "### 1.3: Grid Search (4 marks)\n",
    "\n",
    "For the grid search, we want to compare the performance of the Random Forest model to a Ridge Regression model with the two different column transformers. Think about if we need to use scaling for both models. Select parameter values to test that make sense for both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "5ee11c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Create parameter grid and initialize grid object (3 marks)\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "param_grid = [{'classifier': [KNN()],\n",
    "              'classifier__n_neighbors': [1, 3, 5, 7, 9]},\n",
    "             {'classifier': [RandomForestClassifier()],\n",
    "              'classifier__n_estimators': [10, 50, 100],\n",
    "              'classifier__max_depth': [None, 10, 20]}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "d4b4e4f3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "\nAll the 70 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n70 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/indexes/base.py\", line 3805, in get_loc\n    return self._engine.get_loc(casted_key)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"index.pyx\", line 167, in pandas._libs.index.IndexEngine.get_loc\n  File \"index.pyx\", line 196, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7081, in pandas._libs.hashtable.PyObjectHashTable.get_item\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7089, in pandas._libs.hashtable.PyObjectHashTable.get_item\nKeyError: 'percent_change_next_weeks_price'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/_indexing.py\", line 364, in _get_column_indices\n    col_idx = all_columns.get_loc(col)\n              ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/indexes/base.py\", line 3812, in get_loc\n    raise KeyError(key) from err\nKeyError: 'percent_change_next_weeks_price'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/base.py\", line 1389, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/pipeline.py\", line 654, in fit\n    Xt = self._fit(X, y, routed_params, raw_params=params)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/pipeline.py\", line 588, in _fit\n    X, fitted_transformer = fit_transform_one_cached(\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/joblib/memory.py\", line 312, in __call__\n    return self.func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/pipeline.py\", line 1551, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/_set_output.py\", line 319, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/base.py\", line 1389, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/compose/_column_transformer.py\", line 993, in fit_transform\n    self._validate_column_callables(X)\n  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/compose/_column_transformer.py\", line 552, in _validate_column_callables\n    transformer_to_input_indices[name] = _get_column_indices(X, columns)\n                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/_indexing.py\", line 372, in _get_column_indices\n    raise ValueError(\"A given column is not a column of the dataframe\") from e\nValueError: A given column is not a column of the dataframe\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[157], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GridSearchCV\n\u001b[1;32m      5\u001b[0m grid \u001b[38;5;241m=\u001b[39m GridSearchCV(pipe, param_grid, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, return_train_score\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 7\u001b[0m \u001b[43mgrid\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1024\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m   1018\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m   1019\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m   1020\u001b[0m     )\n\u001b[1;32m   1022\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m-> 1024\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m   1027\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m   1028\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1571\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1569\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1570\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1571\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1001\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    994\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m!=\u001b[39m n_candidates \u001b[38;5;241m*\u001b[39m n_splits:\n\u001b[1;32m    995\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    996\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcv.split and cv.get_n_splits returned \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    997\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minconsistent results. Expected \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    998\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplits, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_splits, \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m n_candidates)\n\u001b[1;32m    999\u001b[0m     )\n\u001b[0;32m-> 1001\u001b[0m \u001b[43m_warn_or_raise_about_fit_failures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1003\u001b[0m \u001b[38;5;66;03m# For callable self.scoring, the return type is only know after\u001b[39;00m\n\u001b[1;32m   1004\u001b[0m \u001b[38;5;66;03m# calling. If the return type is a dictionary, the error scores\u001b[39;00m\n\u001b[1;32m   1005\u001b[0m \u001b[38;5;66;03m# can now be inserted with the correct key. The type checking\u001b[39;00m\n\u001b[1;32m   1006\u001b[0m \u001b[38;5;66;03m# of out will be done in `_insert_error_scores`.\u001b[39;00m\n\u001b[1;32m   1007\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscoring):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:517\u001b[0m, in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[0;34m(results, error_score)\u001b[0m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_failed_fits \u001b[38;5;241m==\u001b[39m num_fits:\n\u001b[1;32m    511\u001b[0m     all_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    512\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAll the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt is very likely that your model is misconfigured.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can try to debug the error by setting error_score=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    515\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    516\u001b[0m     )\n\u001b[0;32m--> 517\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[1;32m    519\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    520\u001b[0m     some_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    521\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mnum_failed_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed out of a total of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    522\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe score on these train-test partitions for these parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    526\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    527\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: \nAll the 70 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n70 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/indexes/base.py\", line 3805, in get_loc\n    return self._engine.get_loc(casted_key)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"index.pyx\", line 167, in pandas._libs.index.IndexEngine.get_loc\n  File \"index.pyx\", line 196, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7081, in pandas._libs.hashtable.PyObjectHashTable.get_item\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7089, in pandas._libs.hashtable.PyObjectHashTable.get_item\nKeyError: 'percent_change_next_weeks_price'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/_indexing.py\", line 364, in _get_column_indices\n    col_idx = all_columns.get_loc(col)\n              ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/indexes/base.py\", line 3812, in get_loc\n    raise KeyError(key) from err\nKeyError: 'percent_change_next_weeks_price'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/base.py\", line 1389, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/pipeline.py\", line 654, in fit\n    Xt = self._fit(X, y, routed_params, raw_params=params)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/pipeline.py\", line 588, in _fit\n    X, fitted_transformer = fit_transform_one_cached(\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/joblib/memory.py\", line 312, in __call__\n    return self.func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/pipeline.py\", line 1551, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/_set_output.py\", line 319, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/base.py\", line 1389, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/compose/_column_transformer.py\", line 993, in fit_transform\n    self._validate_column_callables(X)\n  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/compose/_column_transformer.py\", line 552, in _validate_column_callables\n    transformer_to_input_indices[name] = _get_column_indices(X, columns)\n                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/_indexing.py\", line 372, in _get_column_indices\n    raise ValueError(\"A given column is not a column of the dataframe\") from e\nValueError: A given column is not a column of the dataframe\n"
     ]
    }
   ],
   "source": [
    "# TO DO: Fit grid object to training data (1 mark)\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid, cv=5, return_train_score=True)\n",
    "\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec58f86",
   "metadata": {},
   "source": [
    "### 1.4: Visualize Results (2 marks)\n",
    "\n",
    "The final step is to print out the results from the grid search. You will need to print out the following items:\n",
    "- Best parameters\n",
    "- Best cross-validation train score \n",
    "- Best cross-validation test score\n",
    "- Test set accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "9acc228e",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GridSearchCV' object has no attribute 'best_params_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[158], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# TO DO: Print the results from the grid search (2 marks)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest params:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[43mgrid\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbest_params_\u001b[49m))\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest cross-validation train score: \u001b[39m\u001b[38;5;132;01m{:.2f}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(grid\u001b[38;5;241m.\u001b[39mcv_results_[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_train_score\u001b[39m\u001b[38;5;124m'\u001b[39m][grid\u001b[38;5;241m.\u001b[39mbest_index_]))\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest cross-validation score: \u001b[39m\u001b[38;5;132;01m{:.2f}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(grid\u001b[38;5;241m.\u001b[39mbest_score_))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GridSearchCV' object has no attribute 'best_params_'"
     ]
    }
   ],
   "source": [
    "# TO DO: Print the results from the grid search (2 marks)\n",
    "\n",
    "print(\"Best params:\\n{}\\n\".format(grid.best_params_))\n",
    "print(\"Best cross-validation train score: {:.2f}\".format(grid.cv_results_['mean_train_score'][grid.best_index_]))\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))\n",
    "print(\"Test-set score: {:.2f}\".format(grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7324c9",
   "metadata": {},
   "source": [
    "### Questions (8 marks)\n",
    "\n",
    "1. Which models did you use scaling for? Why?\n",
    "1. Which model and what parameters produced the best results?\n",
    "1. Was this model a good fit? Why or why not?\n",
    "1. Is there anything else we could do to try to improve model performance? Provide two ideas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a35ff2b",
   "metadata": {},
   "source": [
    "*ANSWER HERE*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd7f37f",
   "metadata": {},
   "source": [
    "### Process Description (4 marks)\n",
    "Please describe the process you used to create your code. Cite any websites or generative AI tools used. You can use the following questions as guidance:\n",
    "1. Where did you source your code?\n",
    "1. In what order did you complete the steps?\n",
    "1. If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?\n",
    "1. Did you have any challenges? If yes, what were they? If not, what helped you to be successful?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee2f4ee",
   "metadata": {},
   "source": [
    "*DESCRIBE YOUR PROCESS HERE - BE SPECIFIC*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1446bc2",
   "metadata": {},
   "source": [
    "## Part 2: Text Data (32 marks)\n",
    "\n",
    "The purpose of this part of the assignment is to practice working with text data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864c0a90",
   "metadata": {},
   "source": [
    "### 2.1: Load data (1 mark)\n",
    "For this task, we will be using the hobbies dataset from the yellowbrick library. More information on the dataset can be found here: https://www.scikit-yb.org/en/latest/api/datasets/hobbies.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "93604d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (77.0.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: yellowbrick in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (1.5)\n",
      "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from yellowbrick) (3.10.1)\n",
      "Requirement already satisfied: scipy>=1.0.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from yellowbrick) (1.15.2)\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from yellowbrick) (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.16.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from yellowbrick) (2.2.2)\n",
      "Requirement already satisfied: cycler>=0.10.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from yellowbrick) (0.12.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (1.3.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/kierstenkonig/Library/Python/3.12/lib/python/site-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/kierstenkonig/Library/Python/3.12/lib/python/site-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (2.9.0.post0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scikit-learn>=1.0.0->yellowbrick) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scikit-learn>=1.0.0->yellowbrick) (3.6.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/kierstenkonig/Library/Python/3.12/lib/python/site-packages (from python-dateutil>=2.7->matplotlib!=3.0.0,>=2.0.2->yellowbrick) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'distutils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[163], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minstall yellowbrick\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# TO DO: Install yellowbrick (0.5 marks)\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01myellowbrick\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_hobbies\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Load the hobbies dataset\u001b[39;00m\n\u001b[1;32m      7\u001b[0m data \u001b[38;5;241m=\u001b[39m load_hobbies()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/yellowbrick/__init__.py:31\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_version, __version_info__\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Import the style management functions\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstyle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrcmod\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m reset_defaults, reset_orig\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstyle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrcmod\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m set_aesthetic, set_style, set_palette\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstyle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpalettes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m color_palette, set_color_codes\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/yellowbrick/style/__init__.py:20\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;03mManage the style and aesthetic of the yellowbrick library.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m##########################################################################\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m## Imports\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m##########################################################################\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolors\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpalettes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrcmod\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/yellowbrick/style/colors.py:33\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01myellowbrick\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m YellowbrickValueError\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Check to see if matplotlib is at least sorta up to date\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdistutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LooseVersion\n\u001b[1;32m     35\u001b[0m mpl_ge_150 \u001b[38;5;241m=\u001b[39m LooseVersion(mpl\u001b[38;5;241m.\u001b[39m__version__) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1.5.0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m##########################################################################\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m## Color Utilities\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m##########################################################################\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'distutils'"
     ]
    }
   ],
   "source": [
    "%pip install setuptools\n",
    "%pip install yellowbrick\n",
    "# TO DO: Install yellowbrick (0.5 marks)\n",
    "from yellowbrick.datasets import load_hobbies\n",
    "\n",
    "# Load the hobbies dataset\n",
    "data = load_hobbies()\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f72f48e",
   "metadata": {},
   "source": [
    "### 2.2 Pre-processing (3 marks)\n",
    "\n",
    "We will need to transform the data from strings to numeric. First, we will transform the data using `CountVectorizer(min_df=5)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "b1deaaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Create CountVectorizer object (0.5 marks)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vect = CountVectorizer(min_df=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "25cbc92d",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'news_headlines'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'news_headlines'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[128], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# TO DO: Fit vectorizer to data (0.5 marks)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m vect\u001b[38;5;241m.\u001b[39mfit(\u001b[43mstock_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnews_headlines\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'news_headlines'"
     ]
    }
   ],
   "source": [
    "# TO DO: Fit vectorizer to data (0.5 marks)\n",
    "\n",
    "vect.fit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c812613c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: What is the length of the vocabulary? (0.5 marks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1696f51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Transform the data (0.5 marks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bedc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: What is the shape of the transformed data? (0.5 marks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7e03a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Split data into training and testing sets (use random_state=0 and 10% of the data for testing) (0.5 marks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d11f4e",
   "metadata": {},
   "source": [
    "### 2.3: Grid Search (5 marks)\n",
    "\n",
    "For the grid search, we want to compare the performance of Logistic Regression for different values of C. Initialize the parameter grid with parameter values that make sense for this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137d6e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Create parameter grid and initialize grid object (2 marks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80113917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Fit grid object to training data (1 mark)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3995afa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Print the results from the grid search (2 marks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31294d74",
   "metadata": {},
   "source": [
    "### 2.4: Additional Model Comparisons (9 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ca2da7",
   "metadata": {},
   "source": [
    "### 2.4.1: Naive Bayes (3 marks)\n",
    "We would like to compare the performance of Logistic Regression with one of the Naive Bayes models. Pick the Naive Bayes model that you think would best suit text data and implement below. Since we are not adjusting hyperparameters, we can use `cross_validate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5b6ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Implement Naive Bayes model with cross-validate (2 marks)\n",
    "\n",
    "\n",
    "# TO DO: Print training and validation accuracies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d505f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Calculate and print test accuracy (1 mark)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236ed1f4",
   "metadata": {},
   "source": [
    "### 2.4.2 Tf-idf (6 marks)\n",
    "\n",
    "To try to improve the results, we can try using Tf-idf to tranform the text data based on the importance of each feature. We will need to use a pipeline and the original data for this section. Use `TfidfVectorizer(min_df=5)` and compare the results for both Logistic Regression and your selected Naive Bayes model. Use the Logistic Regression parameters from the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ddef27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Split the data into training and testing sets (same values as previous section) (1 mark)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b7a2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Implement Pipeline with Tf-idf vectorizer and both Logistic Regression and your selected Naive Bayes model (3 marks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea92cc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Print the results from the grid search (2 marks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82aee78b",
   "metadata": {},
   "source": [
    "### Questions (10 marks)\n",
    "\n",
    "1. Which Naive Bayes model did you pick? Why?\n",
    "1. Which model and what parameters produced the best results?\n",
    "1. Was this model a good fit? Why or why not?\n",
    "1. Is there anything else we could do to try to improve model performance? Provide two ideas (must be different from Part 1).\n",
    "1. Why did we need to implement a pipeline for Tf-idf and not CountVectorizer? What would happen if we didn't use one for Tf-idf?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323788fa",
   "metadata": {},
   "source": [
    "*ANSWER HERE*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40621d7",
   "metadata": {},
   "source": [
    "### Process Description (4 marks)\n",
    "Please describe the process you used to create your code. Cite any websites or generative AI tools used. You can use the following questions as guidance:\n",
    "1. Where did you source your code?\n",
    "1. In what order did you complete the steps?\n",
    "1. If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?\n",
    "1. Did you have any challenges? If yes, what were they? If not, what helped you to be successful?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdc92f7",
   "metadata": {},
   "source": [
    "*DESCRIBE YOUR PROCESS HERE - BE SPECIFIC*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc65ad6",
   "metadata": {},
   "source": [
    "## Part 3: Reflection (2 marks)\n",
    "Include a sentence or two about:\n",
    "- what you liked or disliked,\n",
    "- found interesting, confusing, challangeing, motivating\n",
    "while working on this assignment.\n",
    "\n",
    "\n",
    "*ADD YOUR THOUGHTS HERE*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa9752e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
